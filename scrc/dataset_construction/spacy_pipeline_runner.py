import gc
import multiprocessing
import sys
from pathlib import Path
import glob
from time import sleep
from memory_profiler import profile
import os, psutil

import spacy
from spacy.vocab import Vocab
from tqdm import tqdm
import pandas as pd
import dask.dataframe as dd
from dask.diagnostics import ProgressBar
import configparser
from scrc.dataset_construction.dataset_constructor_component import DatasetConstructorComponent
from root import ROOT_DIR
from scrc.utils.decorators import slack_alert
from scrc.utils.log_utils import get_logger

# import scrc.utils.monkey_patch  # prevent memory leak with pandas

# IMPORTANT: make sure you download these models first with: python -m spacy download de_dep_news_trf
import de_core_news_lg, fr_core_news_lg, it_core_news_lg

from scrc.utils.main_utils import get_file_gen
from scrc.utils.slack_util import post_message_to_slack


# TODO find out how to deal with deadlocks
#  A: prevent deadlock
#  B: restart program when deadlock is detected
#  C: for diagnosis print threaddump

class SpacyPipelineRunner(DatasetConstructorComponent):
    """
    Runs the entire spacy pipeline for each text and saves it into the MongoDB.
    This brings the advantage, that we have the heavy computation done in advance,
    and can then use the spacy objects directly in our analysis.

    Here is a very good resource to reduce memory consumption: https://pythonspeed.com/memory/
    """

    def __init__(self, config: dict):
        super().__init__(config)
        self.logger = get_logger(__name__)

        self.models = {
            'de': 'de_core_news_lg',
            'fr': 'fr_core_news_lg',
            'it': 'it_core_news_lg'
        }
        # tag, pos and lemma are enough for now
        self.disable_pipes = ['senter', 'ner', 'attribute_ruler', 'textcat']
        self.active_model = None

    @staticmethod
    def load_spacy_model(model_name, disable_pipes):
        return spacy.load(model_name, disable=disable_pipes)

    @slack_alert
    def run_pipeline(self):
        self.logger.info("Started running spacy pipeline on the texts")

        for lang in self.languages:
            self.logger.info(f"Started processing language {lang}")
            lang_dir = self.create_dir(self.spacy_subdir, lang)  # output dir
            self.load_language_model(lang, lang_dir)

            processed_file_path = self.data_dir / f"{lang}_spiders_spacied.txt"
            spider_list, message = self.compute_remaining_spiders(processed_file_path)
            self.logger.info(message)

            engine = self.get_engine()
            self.add_column(engine, lang, col_name='num_tokens', data_type='bigint') # add new column for num_tokens

            for spider in spider_list:
                # according to docs you should aim for a partition size of 100MB
                # 1 court decision takes approximately between around 10KB and 100KB of RAM when loaded into memory
                # The spacy doc takes about 25x the size of a court decision
                self.run_spacy_pipeline(engine, spider, lang, lang_dir)
                self.mark_as_processed(processed_file_path, spider)

            self.logger.info(f"Finished processing language {lang}")

        self.logger.info("Finished running spacy pipeline on the texts")

    def load_language_model(self, lang, lang_dir):
        self.logger.info("Loading spacy model")
        self.active_model = self.load_spacy_model(self.models[lang], self.disable_pipes)
        # increase max length for long texts: Can lead to memory allocation errors for parser and ner
        self.active_model.max_length = 3000000
        vocab_path = lang_dir / f"_vocab.spacy"
        if vocab_path.exists():
            vocab = Vocab().from_disk(str(vocab_path), exclude=['vectors'])
            self.active_model.vocab = vocab

    @profile
    def run_spacy_pipeline(self, engine, spider, lang, lang_dir):
        """
        Creates and saves the docs generated by the spacy pipeline.
        """
        self.logger.info(f"Processing spider {spider}")

        dfs = self.select(engine, lang, columns='id, text', where=f"spider='{spider}'")  # stream dfs from the db
        for df in dfs:
            df = df[['text', 'id']]  # reorder the df so that we get the text first and the id after
            tuples = list(df.itertuples(index=False))  # convert df to list of tuples
            # batch_size = max(int(len(texts) / self.num_cpus), 1) # a high batch_size can lead to lots of allocated memory
            docs = tqdm(self.active_model.pipe(tuples, n_process=-1, batch_size=1, as_tuples=True), total=len(tuples))
            num_tokens = []
            self.logger.info("Saving spacy docs to disk")
            for doc, id in docs:
                path = lang_dir / (id + ".spacy")
                doc.to_disk(path, exclude=['tensor'])
                num_tokens.append(len(doc))
            df['num_tokens'] = num_tokens
            columns = ['num_tokens']
            self.logger.info("Saving num_tokens to db")
            self.update(engine, df, lang, columns)

            self.active_model.vocab.to_disk(lang_dir / f"_vocab.spacy", exclude=['vectors'])

            gc.collect()
            sleep(2)  # sleep(2) is required to allow measurement of the garbage collector

        memory_usage = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 3
        message = f"Your running process is currently using {memory_usage:.3f} GB of memory"
        print(message)
        try:
            post_message_to_slack(message)
        except:
            print("Could not send message to slack: ", sys.exc_info())


if __name__ == '__main__':
    config = configparser.ConfigParser()
    config.read(ROOT_DIR / 'config.ini')  # this stops working when the script is called from the src directory!

    spacy_pipeline_runner = SpacyPipelineRunner(config)
    spacy_pipeline_runner.run_pipeline()
