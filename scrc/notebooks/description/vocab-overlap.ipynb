{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the vocabulary overlap of different copora to investigate their similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting questions\n",
    "\n",
    "1. Is the vocabulary overlap higher between similar texts than between more different texts?\n",
    "    - different legal areas\n",
    "    - within court vs between courts\n",
    "    - legal texts vs other texts (Wikipedia, News, Scientific Articles, etc.)\n",
    "2. Is the vocabulary overlap higher when we compare larger corpora than smaller ones?\n",
    "3. Is the vocabulary overlap higher in italian and in french than in German because of less compound words?\n",
    "4. Does the vocabulary overlap get higher in German texts when we split compound words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Make medium post out of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spacy models\n",
    "#!python -m spacy download de_core_news_md \n",
    "#!python -m spacy download fr_core_news_md \n",
    "#!python -m spacy download it_core_news_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "import random\n",
    "from tqdm import tqdm # for progress bars\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "import de_core_news_md, fr_core_news_md, it_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy_model(lang='de', type='core', genre='news', size='md'):\n",
    "    # disable pipelines for faster processing since we only need the vectors\n",
    "    nlp = spacy.load(f\"{lang}_{type}_{genre}_{size}\", disable=['tagger', 'parser', 'attribute_ruler', 'ner', 'textcat'])\n",
    "    nlp.max_length = 2000000 # increase max length for long texts\n",
    "    print(f\"Loaded {lang} model with enabled pipes: {nlp.pipe_names}\")\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/fdn-admin/SwissCourtRulingCorpus/data')\n",
    "csv_dir = data_dir / 'csv'\n",
    "raw_csv_dir = csv_dir / 'raw'\n",
    "clean_csv_dir = csv_dir / 'clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(iterable, chunk_size):\n",
    "    return (iterable[pos: pos + chunk_size] for pos in range(0, len(iterable), chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save(texts, path, nlp):\n",
    "    doc_bin = DocBin(attrs=[\"LEMMA\", \"POS\"], store_user_data=True)\n",
    "    for doc in tqdm(nlp.pipe(texts, n_process=-1, batch_size=1), total=len(texts)):\n",
    "        doc_bin.add(doc)\n",
    "    doc_bin.to_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_path(court, chunk_num, extension=\"spacy\"):\n",
    "    return Path(f\"{court}-{chunk_num}.{extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(court: str, nlp):\n",
    "    \"\"\" yields the docs saved by chunk\"\"\"\n",
    "    chunk_num = 0\n",
    "    while True: # while there are still more chunks to read\n",
    "        chunk_path = get_chunk_path(court, chunk_num)\n",
    "        if chunk_path.exists():\n",
    "            doc_bin = DocBin().from_disk(chunk_path)\n",
    "            yield list(doc_bin.get_docs(nlp.vocab))\n",
    "            chunk_num += 1 # go to the next chunk\n",
    "        else: # if there are no more chunks to read\n",
    "            break # abort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_for_doc(doc: spacy.tokens.Doc) -> set:\n",
    "    \"\"\"\n",
    "    take lemma without underscore for faster computation (int instead of str) \n",
    "    take casefold of lemma to remove capital letters and ß\n",
    "    \"\"\"  \n",
    "    return set([token.lemma_.casefold()\n",
    "                for token in doc \n",
    "                if (not token.is_stop\n",
    "                    and not token.is_punct\n",
    "                    and not token.pos_ in ['NUM', 'SYM', 'X'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(docs_gen) -> set:\n",
    "    \"\"\" Creates the vocab (set of words appearing in the corpus/doc) from the document generator\"\"\"\n",
    "    print(\"Creating the vocabulary\")\n",
    "    vocabs = []\n",
    "    for docs in docs_gen:\n",
    "        vocab = [create_vocab_for_doc(doc) for doc in tqdm(docs)]\n",
    "        vocabs.extend(vocab)\n",
    "             \n",
    "    total_vocab = set()\n",
    "    for vocab in vocabs:\n",
    "        total_vocab |= vocab # make the union of all individual vocabs\n",
    "        \n",
    "    print(f\"Here is a sample of 20 random words from the vocabulary: {list(total_vocab)[:20]}\")\n",
    "    print(f\"The vocabulary contains {len(total_vocab)} words\")\n",
    "    return total_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(df: pd.DataFrame, court: str, nlp, chunk_size = 10000, override=False):\n",
    "    \"\"\" \n",
    "    Creates and saves the docs generated by the spacy pipeline. \n",
    "    \"\"\"\n",
    "    first_chunk_path = get_chunk_path(court, 0)\n",
    "    if override or not first_chunk_path.exists() : # if the first chunked docbin does not exist or we want to override it\n",
    "        print(f\"Running spacy pipeline to create docs for court {court}\")\n",
    "        df = df[df['text'].notna()] # Make sure we don't have any NaN values in the text\n",
    "        texts = df.text.tolist()\n",
    "        chunks = chunker(texts, chunk_size) # make chunks because otherwise the doc_bins get too large\n",
    "        for chunk_num, chunk in enumerate(chunks):\n",
    "            print(f\"Processing chunk {chunk_num}\")\n",
    "            process_and_save(chunk, get_chunk_path(court, chunk_num), nlp)\n",
    "    else:\n",
    "        print(f\"Preprocessed docs already exist at {first_chunk_path}. To calculate again set 'override' to True.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_court(court_name, court_df, nlp, override=False):\n",
    "    \"\"\" Run the necessary steps to create the vocab for a court \"\"\"\n",
    "    assert len(court_df.index) > 0\n",
    "    create_docs(court_df, court_name, nlp, override=override)\n",
    "    docs_gen = read_docs(court_name, nlp)\n",
    "    return create_vocab(docs_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for German texts use compound splitter from dtuggener\n",
    "def compute_vocabulary_overlap(court_name_1, court_df_1, court_name_2, court_df_2, nlp):\n",
    "    result = {\"vocab (num lemmas)\": {}} # prepare result dict\n",
    "        \n",
    "    vocab_1 = run_for_court(court_name_1, court_df_1, nlp)\n",
    "    result[\"vocab (num lemmas)\"][court_name_1] = len(vocab_1)\n",
    "    \n",
    "    if court_name_1 == court_name_2: # if both have the same name we want to test within court\n",
    "        override = True # so we need to override the previously created doc_bins\n",
    "    vocab_2 = run_for_court(court_name_2, court_df_2, nlp, override=override)\n",
    "    result[\"vocab (num lemmas)\"][court_name_2] = len(vocab_2)\n",
    "\n",
    "    union = vocab_1 | vocab_2 # compute union\n",
    "    result['union (num lemmas)'] = len(union)\n",
    "\n",
    "    intersection = vocab_1.intersection(vocab_2) # compute intersection\n",
    "    result['intersection (num lemmas)'] = len(intersection)\n",
    "    \n",
    "    # the overlap is the intersection divided by the mean of the vocab lengths\n",
    "    result['overlap (%)'] = len(intersection) / np.mean([len(vocab_1), len(vocab_2)]) \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded de model with enabled pipes: ['tok2vec', 'morphologizer', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "german = load_spacy_model('de') # load language model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spider</th>\n",
       "      <th>language</th>\n",
       "      <th>canton</th>\n",
       "      <th>court</th>\n",
       "      <th>chamber</th>\n",
       "      <th>date</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_number</th>\n",
       "      <th>file_number_additional</th>\n",
       "      <th>html_url</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CH_BGer</td>\n",
       "      <td>de</td>\n",
       "      <td>CH</td>\n",
       "      <td>CH_BGer</td>\n",
       "      <td>CH_BGer_008</td>\n",
       "      <td>2015-12-17</td>\n",
       "      <td>CH_BGer_008_8C-873-2015_2015-12-17</td>\n",
       "      <td>8C 873/2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.bger.ch/ext/eurospider/live/de/php...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bundesgericht Tribunal fédéral Tribunale fed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CH_BGer</td>\n",
       "      <td>de</td>\n",
       "      <td>CH</td>\n",
       "      <td>CH_BGer</td>\n",
       "      <td>CH_BGer_008</td>\n",
       "      <td>2008-02-04</td>\n",
       "      <td>CH_BGer_008_8C-71-2007_2008-02-04</td>\n",
       "      <td>8C 71/2007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.bger.ch/ext/eurospider/live/de/php...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tribunale federale Tribunal federal {T 0/2} 8C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CH_BGer</td>\n",
       "      <td>de</td>\n",
       "      <td>CH</td>\n",
       "      <td>CH_BGer</td>\n",
       "      <td>CH_BGer_008</td>\n",
       "      <td>2008-08-07</td>\n",
       "      <td>CH_BGer_008_8C-237-2008_2008-08-07</td>\n",
       "      <td>8C 237/2008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.bger.ch/ext/eurospider/live/de/php...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tribunale federale Tribunal federal {T 0/2} 8C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CH_BGer</td>\n",
       "      <td>de</td>\n",
       "      <td>CH</td>\n",
       "      <td>CH_BGer</td>\n",
       "      <td>CH_BGer_004</td>\n",
       "      <td>2007-05-16</td>\n",
       "      <td>CH_BGer_004_4A-32-2007_2007-05-16</td>\n",
       "      <td>4A 32/2007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.bger.ch/ext/eurospider/live/de/php...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tribunale federale Tribunal federal {T 0/2} 4A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CH_BGer</td>\n",
       "      <td>de</td>\n",
       "      <td>CH</td>\n",
       "      <td>CH_BGer</td>\n",
       "      <td>CH_BGer_011</td>\n",
       "      <td>2019-04-12</td>\n",
       "      <td>CH_BGer_011_6B-224-2019_2019-04-12</td>\n",
       "      <td>6B 224/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.bger.ch/ext/eurospider/live/de/php...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bundesgericht Tribunal fédéral Tribunale fed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     spider language canton    court      chamber        date  \\\n",
       "0   CH_BGer       de     CH  CH_BGer  CH_BGer_008  2015-12-17   \n",
       "1   CH_BGer       de     CH  CH_BGer  CH_BGer_008  2008-02-04   \n",
       "3   CH_BGer       de     CH  CH_BGer  CH_BGer_008  2008-08-07   \n",
       "11  CH_BGer       de     CH  CH_BGer  CH_BGer_004  2007-05-16   \n",
       "12  CH_BGer       de     CH  CH_BGer  CH_BGer_011  2019-04-12   \n",
       "\n",
       "                             file_name  file_number  file_number_additional  \\\n",
       "0   CH_BGer_008_8C-873-2015_2015-12-17  8C 873/2015                     NaN   \n",
       "1    CH_BGer_008_8C-71-2007_2008-02-04   8C 71/2007                     NaN   \n",
       "3   CH_BGer_008_8C-237-2008_2008-08-07  8C 237/2008                     NaN   \n",
       "11   CH_BGer_004_4A-32-2007_2007-05-16   4A 32/2007                     NaN   \n",
       "12  CH_BGer_011_6B-224-2019_2019-04-12  6B 224/2019                     NaN   \n",
       "\n",
       "                                             html_url  pdf_url  \\\n",
       "0   https://www.bger.ch/ext/eurospider/live/de/php...      NaN   \n",
       "1   https://www.bger.ch/ext/eurospider/live/de/php...      NaN   \n",
       "3   https://www.bger.ch/ext/eurospider/live/de/php...      NaN   \n",
       "11  https://www.bger.ch/ext/eurospider/live/de/php...      NaN   \n",
       "12  https://www.bger.ch/ext/eurospider/live/de/php...      NaN   \n",
       "\n",
       "                                                 text  \n",
       "0   Bundesgericht Tribunal fédéral Tribunale fed...  \n",
       "1   Tribunale federale Tribunal federal {T 0/2} 8C...  \n",
       "3   Tribunale federale Tribunal federal {T 0/2} 8C...  \n",
       "11  Tribunale federale Tribunal federal {T 0/2} 4A...  \n",
       "12  Bundesgericht Tribunal fédéral Tribunale fed...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(clean_csv_dir / f\"CH_BGer.csv\") # read df\n",
    "df = df[df.language.str.contains('de')] # select only german documents\n",
    "df = df[df.court.notna()] # remove courts which are NA\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed docs already exist at CH_BGer_004-0.spacy. To calculate again set 'override' to True.\n",
      "Creating the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 684.01it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a sample of 20 random words from the vocabulary: ['law', 'auslegungsregeln', 'luterbacher', 'gegensätzlichen', 'wirteverband', 'ruch', 'anpreisen', 'verkehrsschutzes', 'ursprungsbezeichnung', 'wettbewerbsrechtlich', 'zvd-werte', 'sinken', 'konzernverhalten', 'vorleistungspflicht', 'definierbarer', 'wortfolge', 'mängelfreie', 'verfügen', 'reaktionszeit', 'routinemassnahmen']\n",
      "The vocabulary contains 46716 words\n",
      "Running spacy pipeline to create docs for court CH_BGer_004\n",
      "Processing chunk 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:03<00:00, 15.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 624.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a sample of 20 random words from the vocabulary: ['law', 'auslegungsregeln', 'gegensätzlichen', 'feststellungsentscheid', 'monatelang', \"18'794'115.78\", 'anpreisen', 'verkehrsschutzes', 'ursprungsbezeichnung', 'wettbewerbsrechtlich', 'eintragungsprinzip', 'immobilienangebot', 'berufungsduplik', 'zinsfreien', 'sinken', 'ionisierend', 'nichteinreichen', 'vorleistungspflicht', 'mängelfreie', 'wortfolge']\n",
      "The vocabulary contains 48232 words\n",
      "CPU times: user 41.3 s, sys: 4.56 s, total: 45.9 s\n",
      "Wall time: 1min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vocab (num lemmas)': {'CH_BGer_004': 48232},\n",
       " 'union (num lemmas)': 68057,\n",
       " 'intersection (num lemmas)': 26891,\n",
       " 'overlap (%)': 0.5664363651683026}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "court_name_1 = 'CH_BGer_004'\n",
    "court_df_1 = df[df.chamber.str.contains(court_name_1)] # select court\n",
    "court_df_1 = court_df_1.sample(1000, random_state=seed, axis=0)\n",
    "\n",
    "court_name_2 = 'CH_BGer_004'\n",
    "court_df_2 = df[df.chamber.str.contains(court_name_2)] # select court\n",
    "court_df_2 = court_df_2.sample(1000, random_state=seed+1, axis=0)\n",
    "\n",
    "result = compute_vocabulary_overlap(court_name_1, court_df_1, court_name_2, court_df_2, german)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#result = compute_vocabulary_overlap('TI_PP', 'TI_TCA', lang='it')\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
